{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "HW2 coding part.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlOXgQ3dIauY"
      },
      "source": [
        "# HW 2 coding part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKbehbQTIauc"
      },
      "source": [
        "Due: SEP 29 AT 11:59PM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AlfBuXIIaue"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EdtrGQIauu"
      },
      "source": [
        "# Problem 1. KNN and Spam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61_xqKPXIauv"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnKFq-YtIauy"
      },
      "source": [
        "The dataset consists of a collection of 57 features relating to about 4600 emails and a label of whether or not the email is considered spam. You have a training set containing about 70% of the data and a test set containing about 30% of the data. Your job is to build effective spam classification rules using the predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ_GOhjgIau0"
      },
      "source": [
        "## A note about features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YIR-0QfIau2"
      },
      "source": [
        "The column names (in the first row of each .csv file) are fairly self-explanatory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB01_d8UIau4"
      },
      "source": [
        "Some variables are named `word_freq_(word)`, which suggests a calculation of the frequency of how many times a specific word appears in the email, expressed as a percentage of total words in the email multiplied by $100$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWx0pRZNIau7"
      },
      "source": [
        "Some variables are named `char_freq_(*)`, which suggests a count of the frequency of the specific ensuing character, expressed as a percentage of total characters in the email multiplied by $100$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTlssQPAIau8"
      },
      "source": [
        "Some variables are named `capital_run_length_(*)` which suggests some information about the average (or maximum length of, or total) consecutive capital letters in the email."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_fjm3YIau-"
      },
      "source": [
        "`spam`: This is the response variable, 0 = not spam, 1 = spam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djwJKXAQIau_"
      },
      "source": [
        "dftr = pd.read_csv('spam_train_withlabels.csv',header=0) #(3721, 58)\n",
        "dfte = pd.read_csv('spam_test_nolabels.csv',header=0) #(880, 57)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83FGAzsvIavS"
      },
      "source": [
        "vv = pd.concat([dftr.drop('spam',axis=1),dfte])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dtVXgerIavq",
        "outputId": "95f704b6-3888-41e0-fb46-6a2a9e099a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(np.unique(vv.index,return_counts=True)[1]>1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80QtlH93Iavu"
      },
      "source": [
        "## Missing Values and outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0qyNavmIavv"
      },
      "source": [
        "This time look for missing values and outliers in the `capital_run_length_average` column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWAEhCkfIavv"
      },
      "source": [
        "You can view missing values with (recall that an NA or `np.nan`) represent missing values. **DO NOT DELETE THEM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Rz3xa0MmPH"
      },
      "source": [
        "ANS: There're 341"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aXke6TKFIavw",
        "outputId": "9fdd6ac7-c9d5-4e7c-e85f-e598135f49ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Missing value in spam_train_withlabels.csv: \",dftr.isna().sum().sort_values(ascending=False)['capital_run_length_average'], \";\",\n",
        "      \"Missing value in spam_train_nolabels.csv: \",dfte.isna().sum().sort_values(ascending=False)['capital_run_length_average'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing value in spam_train_withlabels.csv:  341 ; Missing value in spam_train_nolabels.csv:  63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo-8JEJAIavz"
      },
      "source": [
        "### Task 0\n",
        "Output the total number of outlier values that you have clearly found. Explain your reasoning. **DO NOT DELETE THEM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KQ8uMCyPWva",
        "outputId": "b0eb7d8b-96dd-434d-a476-78012742a64c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "# return binary column name\n",
        "dftr.T[dftr.isin([0,1]).all()].T.columns\n",
        "\n",
        "# exclude the binary column in the outlier value counting process\n",
        "for column in dftr.columns[0:57]:\n",
        "  outlier_num = 0\n",
        "  col = dftr[column]\n",
        "  outlier_num = ((col - col.mean()).abs() > 3 * col.std()).value_counts()[1]\n",
        "  print(column, \" has \", outlier_num, \"outliers\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_freq_make  has  68 outliers\n",
            "word_freq_address  has  31 outliers\n",
            "word_freq_all  has  77 outliers\n",
            "word_freq_3d  has  12 outliers\n",
            "word_freq_our  has  70 outliers\n",
            "word_freq_over  has  80 outliers\n",
            "word_freq_remove  has  89 outliers\n",
            "word_freq_internet  has  55 outliers\n",
            "word_freq_order  has  90 outliers\n",
            "word_freq_mail  has  56 outliers\n",
            "word_freq_receive  has  79 outliers\n",
            "word_freq_will  has  84 outliers\n",
            "word_freq_people  has  69 outliers\n",
            "word_freq_report  has  80 outliers\n",
            "word_freq_addresses  has  79 outliers\n",
            "word_freq_free  has  60 outliers\n",
            "word_freq_business  has  86 outliers\n",
            "word_freq_email  has  86 outliers\n",
            "word_freq_you  has  48 outliers\n",
            "word_freq_credit  has  62 outliers\n",
            "word_freq_your  has  67 outliers\n",
            "word_freq_font  has  46 outliers\n",
            "word_freq_000  has  87 outliers\n",
            "word_freq_money  has  25 outliers\n",
            "word_freq_hp  has  69 outliers\n",
            "word_freq_hpl  has  85 outliers\n",
            "word_freq_george  has  101 outliers\n",
            "word_freq_650  has  90 outliers\n",
            "word_freq_lab  has  50 outliers\n",
            "word_freq_labs  has  67 outliers\n",
            "word_freq_telnet  has  54 outliers\n",
            "word_freq_857  has  40 outliers\n",
            "word_freq_data  has  61 outliers\n",
            "word_freq_415  has  41 outliers\n",
            "word_freq_85  has  60 outliers\n",
            "word_freq_technology  has  61 outliers\n",
            "word_freq_1999  has  84 outliers\n",
            "word_freq_parts  has  16 outliers\n",
            "word_freq_pm  has  57 outliers\n",
            "word_freq_direct  has  49 outliers\n",
            "word_freq_cs  has  46 outliers\n",
            "word_freq_meeting  has  64 outliers\n",
            "word_freq_original  has  89 outliers\n",
            "word_freq_project  has  42 outliers\n",
            "word_freq_re  has  54 outliers\n",
            "word_freq_edu  has  58 outliers\n",
            "word_freq_table  has  21 outliers\n",
            "word_freq_conference  has  44 outliers\n",
            "char_freq_;  has  20 outliers\n",
            "char_freq_(  has  38 outliers\n",
            "char_freq_[  has  20 outliers\n",
            "char_freq_!  has  36 outliers\n",
            "char_freq_$  has  42 outliers\n",
            "char_freq_#  has  22 outliers\n",
            "capital_run_length_average  has  5 outliers\n",
            "capital_run_length_longest  has  34 outliers\n",
            "capital_run_length_total  has  72 outliers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NA6NiY3RvwF"
      },
      "source": [
        "EXPLANATION: The total number of outlier values that I have found has been printed above. I first exclued the column with binary values and count outlier values among non-binary columns. Any column with z-score greater than 3 or less than -3 is considered to be an outlier. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM5jEzbTIav2"
      },
      "source": [
        "### Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfYh2nkjIav2"
      },
      "source": [
        "Split off the `spam` column into a new vector called `ytrain` and drop the column from the training dataframe. Use $k$-nearest neighbors regression **trained on the training dataset**--there are times when one could train on both test and train, but let's focus on train for now c.f. inductive learning--with $k = 15$ to impute the missing/outlier values in the `capital_run_length_average` column using the other predictors after standardizing (i.e. rescaling) them. You are allowed to just use the `sklearn` neighbors module to perform this. (Take a look at the pandas `concat` command and remember the `drop` command from the last HW.) Remember to rescale using information from the training set. Rescale the test set also."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMGTFJAdIav2"
      },
      "source": [
        "When you are done with this part, you should have no more NA’s in the capital_run_length_average column in either the training or the test set. The mean (standard deviation) of the training dataset should be 0 (resp 1). The test should should be close to that, but not exactly that since you are rescaling based on the training data. There are times when it might make sense to scale based on the testing data (like if you assume the distribution has shifted a little bit), but we aren't doing that. Make sure you show all of your work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh_nGboX44eI"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "dlra = 'capital_run_length_average'\n",
        "ytrain = dftr['spam']\n",
        "xtrain = dftr.drop('spam', 1)\n",
        "xtest = dfte"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6T1TDvdazwp",
        "outputId": "88bfd10c-153e-4385-e8fe-95aeb7bddfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "clf = KNeighborsRegressor(15)\n",
        "impute = xtrain.copy()\n",
        "impute.dropna(inplace = True)\n",
        "cap_run_avg = impute[dlra]\n",
        "impute.drop(dlra, 1, inplace = True)\n",
        "for name, data in impute.iteritems():\n",
        "    impute[name] = (data-data.mean())/data.std()\n",
        "\n",
        "clf.fit(impute, cap_run_avg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                    metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
              "                    weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhxf5WU7tSE6"
      },
      "source": [
        "xtrain.drop(dlra, 1, inplace = True)\n",
        "xtest.drop(dlra, 1, inplace = True)\n",
        "\n",
        "xtrain[dlra] = clf.predict(xtrain)\n",
        "xtest[dlra] = clf.predict(xtest)\n",
        "\n",
        "xtrain = (xtrain - xtrain.mean())/xtrain.std()\n",
        "xtest = (xtest - xtest.mean())/xtest.std()\n",
        "\n",
        "impute_test_dlra = xtest[dlra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EJ8EzBt7n2h",
        "outputId": "bec9606b-2689-4c27-a757-e24d44bdb973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "xtrain.mean(), xtrain.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(word_freq_make               -6.188725e-16\n",
              " word_freq_address            -2.968750e-16\n",
              " word_freq_all                -2.508668e-16\n",
              " word_freq_3d                 -3.963505e-16\n",
              " word_freq_our                 2.139290e-17\n",
              " word_freq_over                7.480057e-17\n",
              " word_freq_remove              1.207908e-15\n",
              " word_freq_internet            6.685506e-16\n",
              " word_freq_order              -3.535647e-17\n",
              " word_freq_mail                3.819991e-16\n",
              " word_freq_receive             5.794284e-16\n",
              " word_freq_will                3.429727e-16\n",
              " word_freq_people              3.469410e-16\n",
              " word_freq_report             -4.906792e-16\n",
              " word_freq_addresses          -1.645195e-16\n",
              " word_freq_free               -5.698508e-16\n",
              " word_freq_business           -4.699576e-16\n",
              " word_freq_email               4.946922e-16\n",
              " word_freq_you                -1.744223e-15\n",
              " word_freq_credit              6.187084e-16\n",
              " word_freq_your                1.429058e-15\n",
              " word_freq_font                2.697609e-16\n",
              " word_freq_000                 4.269331e-16\n",
              " word_freq_money               2.354860e-16\n",
              " word_freq_hp                  4.562775e-17\n",
              " word_freq_hpl                -3.262194e-16\n",
              " word_freq_george              1.764094e-16\n",
              " word_freq_650                 5.549623e-18\n",
              " word_freq_lab                 1.449019e-16\n",
              " word_freq_labs               -8.227466e-17\n",
              " word_freq_telnet             -4.773869e-19\n",
              " word_freq_857                -4.141332e-16\n",
              " word_freq_data                9.835215e-16\n",
              " word_freq_415                -4.016018e-17\n",
              " word_freq_85                  1.247024e-16\n",
              " word_freq_technology         -1.392776e-16\n",
              " word_freq_1999                5.704774e-17\n",
              " word_freq_parts              -6.619940e-16\n",
              " word_freq_pm                 -5.676429e-17\n",
              " word_freq_direct             -5.909155e-16\n",
              " word_freq_cs                  2.927575e-16\n",
              " word_freq_meeting            -3.243994e-17\n",
              " word_freq_original            5.904680e-16\n",
              " word_freq_project            -4.088670e-16\n",
              " word_freq_re                  3.285019e-16\n",
              " word_freq_edu                -7.463796e-16\n",
              " word_freq_table              -7.713753e-16\n",
              " word_freq_conference         -4.630653e-16\n",
              " char_freq_;                   1.056890e-15\n",
              " char_freq_(                   7.808540e-16\n",
              " char_freq_[                   4.751790e-16\n",
              " char_freq_!                   1.165719e-16\n",
              " char_freq_$                  -9.840138e-17\n",
              " char_freq_#                  -4.264483e-16\n",
              " capital_run_length_longest   -2.000550e-17\n",
              " capital_run_length_total      1.212115e-17\n",
              " capital_run_length_average   -8.330402e-16\n",
              " dtype: float64, word_freq_make                1.0\n",
              " word_freq_address             1.0\n",
              " word_freq_all                 1.0\n",
              " word_freq_3d                  1.0\n",
              " word_freq_our                 1.0\n",
              " word_freq_over                1.0\n",
              " word_freq_remove              1.0\n",
              " word_freq_internet            1.0\n",
              " word_freq_order               1.0\n",
              " word_freq_mail                1.0\n",
              " word_freq_receive             1.0\n",
              " word_freq_will                1.0\n",
              " word_freq_people              1.0\n",
              " word_freq_report              1.0\n",
              " word_freq_addresses           1.0\n",
              " word_freq_free                1.0\n",
              " word_freq_business            1.0\n",
              " word_freq_email               1.0\n",
              " word_freq_you                 1.0\n",
              " word_freq_credit              1.0\n",
              " word_freq_your                1.0\n",
              " word_freq_font                1.0\n",
              " word_freq_000                 1.0\n",
              " word_freq_money               1.0\n",
              " word_freq_hp                  1.0\n",
              " word_freq_hpl                 1.0\n",
              " word_freq_george              1.0\n",
              " word_freq_650                 1.0\n",
              " word_freq_lab                 1.0\n",
              " word_freq_labs                1.0\n",
              " word_freq_telnet              1.0\n",
              " word_freq_857                 1.0\n",
              " word_freq_data                1.0\n",
              " word_freq_415                 1.0\n",
              " word_freq_85                  1.0\n",
              " word_freq_technology          1.0\n",
              " word_freq_1999                1.0\n",
              " word_freq_parts               1.0\n",
              " word_freq_pm                  1.0\n",
              " word_freq_direct              1.0\n",
              " word_freq_cs                  1.0\n",
              " word_freq_meeting             1.0\n",
              " word_freq_original            1.0\n",
              " word_freq_project             1.0\n",
              " word_freq_re                  1.0\n",
              " word_freq_edu                 1.0\n",
              " word_freq_table               1.0\n",
              " word_freq_conference          1.0\n",
              " char_freq_;                   1.0\n",
              " char_freq_(                   1.0\n",
              " char_freq_[                   1.0\n",
              " char_freq_!                   1.0\n",
              " char_freq_$                   1.0\n",
              " char_freq_#                   1.0\n",
              " capital_run_length_longest    1.0\n",
              " capital_run_length_total      1.0\n",
              " capital_run_length_average    1.0\n",
              " dtype: float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFZYTvak5L1F",
        "outputId": "1976ea95-5d45-4d48-a225-d20fb2d2b5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "xtest.mean(), xtest.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(word_freq_make               -4.107825e-16\n",
              " word_freq_address            -5.090625e-17\n",
              " word_freq_all                 4.030867e-16\n",
              " word_freq_3d                  9.323350e-17\n",
              " word_freq_our                -1.404180e-16\n",
              " word_freq_over               -6.926278e-17\n",
              " word_freq_remove              3.046805e-16\n",
              " word_freq_internet            1.261617e-17\n",
              " word_freq_order               4.061145e-16\n",
              " word_freq_mail                3.041759e-16\n",
              " word_freq_receive            -1.378947e-16\n",
              " word_freq_will                5.875351e-16\n",
              " word_freq_people              8.578996e-18\n",
              " word_freq_report              1.877286e-16\n",
              " word_freq_addresses          -1.108331e-16\n",
              " word_freq_free                5.588964e-17\n",
              " word_freq_business            2.985617e-16\n",
              " word_freq_email               2.527650e-16\n",
              " word_freq_you                 2.003826e-15\n",
              " word_freq_credit              1.468522e-16\n",
              " word_freq_your                1.680632e-16\n",
              " word_freq_font                2.025211e-16\n",
              " word_freq_000                 3.178013e-16\n",
              " word_freq_money              -9.638754e-17\n",
              " word_freq_hp                 -1.675427e-16\n",
              " word_freq_hpl                 9.537825e-17\n",
              " word_freq_george              6.308085e-20\n",
              " word_freq_650                -5.664661e-17\n",
              " word_freq_lab                -9.872154e-17\n",
              " word_freq_labs               -1.766264e-17\n",
              " word_freq_telnet              3.759619e-17\n",
              " word_freq_857                 2.624164e-17\n",
              " word_freq_data                6.251313e-17\n",
              " word_freq_415                 5.141090e-17\n",
              " word_freq_85                  5.771898e-17\n",
              " word_freq_technology          3.301652e-16\n",
              " word_freq_1999                1.633794e-16\n",
              " word_freq_parts               6.916500e-16\n",
              " word_freq_pm                  1.148072e-17\n",
              " word_freq_direct             -1.816098e-16\n",
              " word_freq_cs                 -6.390090e-17\n",
              " word_freq_meeting             1.168257e-16\n",
              " word_freq_original            4.358887e-17\n",
              " word_freq_project            -5.721433e-17\n",
              " word_freq_re                  1.177089e-16\n",
              " word_freq_edu                 1.924597e-16\n",
              " word_freq_table               7.065056e-17\n",
              " word_freq_conference         -3.948861e-17\n",
              " char_freq_;                  -1.620547e-16\n",
              " char_freq_(                   3.442953e-16\n",
              " char_freq_[                   5.544492e-16\n",
              " char_freq_!                   1.252786e-16\n",
              " char_freq_$                   6.207156e-17\n",
              " char_freq_#                  -4.547499e-16\n",
              " capital_run_length_longest    1.450860e-18\n",
              " capital_run_length_total     -6.686570e-18\n",
              " capital_run_length_average   -8.483665e-16\n",
              " dtype: float64, word_freq_make                1.0\n",
              " word_freq_address             1.0\n",
              " word_freq_all                 1.0\n",
              " word_freq_3d                  1.0\n",
              " word_freq_our                 1.0\n",
              " word_freq_over                1.0\n",
              " word_freq_remove              1.0\n",
              " word_freq_internet            1.0\n",
              " word_freq_order               1.0\n",
              " word_freq_mail                1.0\n",
              " word_freq_receive             1.0\n",
              " word_freq_will                1.0\n",
              " word_freq_people              1.0\n",
              " word_freq_report              1.0\n",
              " word_freq_addresses           1.0\n",
              " word_freq_free                1.0\n",
              " word_freq_business            1.0\n",
              " word_freq_email               1.0\n",
              " word_freq_you                 1.0\n",
              " word_freq_credit              1.0\n",
              " word_freq_your                1.0\n",
              " word_freq_font                1.0\n",
              " word_freq_000                 1.0\n",
              " word_freq_money               1.0\n",
              " word_freq_hp                  1.0\n",
              " word_freq_hpl                 1.0\n",
              " word_freq_george              1.0\n",
              " word_freq_650                 1.0\n",
              " word_freq_lab                 1.0\n",
              " word_freq_labs                1.0\n",
              " word_freq_telnet              1.0\n",
              " word_freq_857                 1.0\n",
              " word_freq_data                1.0\n",
              " word_freq_415                 1.0\n",
              " word_freq_85                  1.0\n",
              " word_freq_technology          1.0\n",
              " word_freq_1999                1.0\n",
              " word_freq_parts               1.0\n",
              " word_freq_pm                  1.0\n",
              " word_freq_direct              1.0\n",
              " word_freq_cs                  1.0\n",
              " word_freq_meeting             1.0\n",
              " word_freq_original            1.0\n",
              " word_freq_project             1.0\n",
              " word_freq_re                  1.0\n",
              " word_freq_edu                 1.0\n",
              " word_freq_table               1.0\n",
              " word_freq_conference          1.0\n",
              " char_freq_;                   1.0\n",
              " char_freq_(                   1.0\n",
              " char_freq_[                   1.0\n",
              " char_freq_!                   1.0\n",
              " char_freq_$                   1.0\n",
              " char_freq_#                   1.0\n",
              " capital_run_length_longest    1.0\n",
              " capital_run_length_total      1.0\n",
              " capital_run_length_average    1.0\n",
              " dtype: float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euvg0xP6Iav6"
      },
      "source": [
        "### Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HFI9r0tIav6"
      },
      "source": [
        "Write a function named `knnlearn()` that performs $k$-nearest neighbors classification, without resorting to a package. You do not  need to implement a fancy nearest neighbor search algorithm, just the naive search will suffice. Use the euclidean norm. Your function will implement some additional behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XmxWQ8_Iav7"
      },
      "source": [
        "-  The function should automatically do a split of the training data into a sub-training set (80%) and a validation set (20%) for selecting the optimal $k$.\n",
        "- The function should standardize each column: for a particular variable, say `x1`, compute the mean and standard deviation of `x1` **using the training set only**, say `mu1` and `s1`; then for each observed column in the training set and test set, subtract the respective `mu` and divide by the respective `s`. Your data should already be standardized, but now this code does it automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgJ9Ku3vQ9vk"
      },
      "source": [
        "def euclid_dist(a, b):\n",
        "  return np.sum((a-b)**2,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd0GBFzcuqKv"
      },
      "source": [
        "def knn_predict(x_train, x_test, y_train, num_neighbors):\n",
        "  dist = []\n",
        "  neigh_ind = []\n",
        "\n",
        "  dists = [euclid_dist(i, x_train) for j, i in x_test.iterrows()]\n",
        "\n",
        "  for row in dists:\n",
        "    enum_neigh = enumerate(row)\n",
        "    sorted_neigh = sorted(enum_neigh,\n",
        "                          key=lambda x: x[1])[:num_neighbors]\n",
        "\n",
        "    ind_list = [tup[0] for tup in sorted_neigh]\n",
        "    dist_list = [tup[1] for tup in sorted_neigh]\n",
        "\n",
        "    dist.append(dist_list)\n",
        "    neigh_ind.append(ind_list)\n",
        "\n",
        "  neighbors = np.array(neigh_ind)\n",
        "\n",
        "  y_pred = np.array([np.argmax(np.bincount(y_train.take(neighbor))) for neighbor in neighbors])\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX1HyXFluGye"
      },
      "source": [
        "def knn_eval_score(x_train, x_test, y_train, y_test, num_neighbors):\n",
        "  y_pred = knn_predict(x_train, x_test, y_train, num_neighbors)\n",
        "\n",
        "  return np.sum(y_pred == y_test) / len(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70yqKRomIav8"
      },
      "source": [
        "def knnlearn(xtrain,xtest,ytrain, max_k = 5):\n",
        "    num_neighbors = range(2, max_k + 1)\n",
        "    mask = np.random.rand(len(xtrain)) < 0.8\n",
        "    xtrain_train = xtrain[mask]\n",
        "    xtrain_test = xtrain[~mask]\n",
        "    ytrain_train = ytrain[mask]\n",
        "    ytrain_test = ytrain[~mask]\n",
        "    \"\"\"\n",
        "    your code for producing a nearest neighbor estimate\n",
        "    \"\"\"\n",
        "    for i in xtrain_train.columns:\n",
        "      mu = np.mean(xtrain_train[i])\n",
        "      s = np.std(xtrain_train[i])\n",
        "      xtrain_train[i] = (xtrain_train[i] - mu) / s\n",
        "      xtrain_test[i] = (xtrain_test[i] - mu) / s\n",
        "    \n",
        "    best_k = {'k': 0, 'score': 0}\n",
        "\n",
        "    for i in num_neighbors:\n",
        "      score = knn_eval_score(xtrain_train, xtrain_test, ytrain_train, ytrain_test, i)\n",
        "      if score > best_k['score']:\n",
        "        best_k['score'] = score\n",
        "        best_k['k'] = i\n",
        "\n",
        "    print(best_k)\n",
        "\n",
        "    y_pred = knn_predict(xtrain_train, xtest, ytrain_train, best_k['k'])\n",
        "\n",
        "    return y_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNVYEDMIIav_"
      },
      "source": [
        "### Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML3lSyqDIav_"
      },
      "source": [
        "In this part, you will use your k-NN classifier to fit models on the actual dataset. If you weren’t able to successfully write a k-NN classifier in `Task 2`, you’re permitted to use `sklearn`. If you take this route, you may need to write some code to standardize the variables and select k, which `knnlearn()` from `Task 2` already does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvPqShYIawA"
      },
      "source": [
        "Now fit 2 models and produce 2 sets of predictions of spam on the test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAR8XoTNIawB"
      },
      "source": [
        "1. `knnlearn()` using all predictors except for `capital_run_length_average` (say, if we were distrustful of our imputation approach). Call these predictions `knn_pred1`.\n",
        "\n",
        "2. `knnlearn()` using all predictors including `capital_run_length_average` with the imputed values. Call these predictions `knn_pred2`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxxflWwOIawB"
      },
      "source": [
        "Submit a `.csv` file called `assn2_NETID_results.csv` (to Canvas, **NOT** Gradescope) with the columns:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJum2YjCIawC"
      },
      "source": [
        "1. `capital_run_length_average`: the predictor in your test set that now contains the imputed values (so that we can check your work on imputation).\n",
        "2. `knn_pred1`\n",
        "3. `knn_pred2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eHoNj9dIawC"
      },
      "source": [
        "Make sure that row 1 here corresponds to row 1 of the test set, row 2 corresponds to row 2 of the test set, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOfzeec6pNLk",
        "outputId": "09a79f27-8f5a-49c1-82b2-0c9f7b30fa9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "y_train_clean = dftr['spam']\n",
        "x_train_clean = dftr.drop(['spam',dlra], 1)\n",
        "x_test_clean = dfte.drop(dlra, 1)\n",
        "\n",
        "knn_pred1 = knnlearn(x_train_clean, x_test_clean, y_train_clean, max_k = 30)\n",
        "knn_pred2 = knnlearn(xtrain,xtest,ytrain, max_k = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'k': 9, 'score': 0.8972972972972973}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpPb36q3vr1_"
      },
      "source": [
        "csv_df = pd.DataFrame({'Capital_Run_Length_Average':impute_test_dlra, 'knn_pred1': knn_pred1, 'knn_pred2': knn_pred2})\n",
        "csv_df.to_csv('assn2_NETID_results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzAj8YwOIawD"
      },
      "source": [
        "# Problem 2. Methods to solve Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4se-Su7IawD"
      },
      "source": [
        "We saw that the solution to least squares is $\\hat{\\beta} = (X^T X)^{-1} X^T y$. There is no such closed form solution for logistic regression. Recall that for logistic regression (when $y_i \\in \\{0,1\\}$)\n",
        "\n",
        "$$\n",
        "\\hat{\\beta} = \\arg \\min_{\\beta} f(\\beta)\n",
        "$$\n",
        "where\n",
        "$$\n",
        "f(\\beta) = \\frac{1}{n} \\sum_{i=1}^n y_i x_i^T \\beta - \\log(1+\\exp(x_i^T \\beta))\n",
        "$$\n",
        "Later on we will explore various methods to solve such problems. For now, one method is an interative method where we start with $\\beta_0 = 0$ and update based on\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - [H f(\\beta_k)]^{-1} \\nabla f(\\beta_k)\n",
        "$$\n",
        "where\n",
        "$$\n",
        "[H f(\\beta_k)]_{ij} = \\frac{\\partial^2 f(\\beta_k)}{\\partial_i \\partial_j}\n",
        "$$\n",
        "is the matrix of second derivatives also known as the Hessian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx1xliIeJUl4"
      },
      "source": [
        "$$\n",
        "\\nabla f(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(-Y_{i} X_{i}^{T}+\\frac{1}{1+e^{X_{i}^{T} \\beta}} * e^{X_{i}^{T} \\beta} * X_{i}^{T}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiGbctT5IawD"
      },
      "source": [
        "### Task 1\n",
        "Write a function to implement 100 iterations of the above method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCM4DAxXIawE"
      },
      "source": [
        "import numpy as np\n",
        "def logreg(X,y):\n",
        "    \"\"\"\n",
        "    input\n",
        "    X: 2-d numpy array that is n by p\n",
        "    y: numpy array of 0 and 1 of length n\n",
        "    output\n",
        "    B:\n",
        "    2-d numpy array that is p by 100 where the $i^{th}$ column is beta_i.\n",
        "    By definition the first (0^{th} in numpy) column of B should be all zeros.\n",
        "    \"\"\"\n",
        "    n,p = X.shape ##python unpackaging a tuple\n",
        "    assert n==y.shape[0]    \n",
        "    B = np.zeros([p, 100])\n",
        "    for j in range(1,100):\n",
        "        fb = np.sum(np.dot(B[:, j - 1], X.T) * y - np.log(1 + np.exp(np.dot(B[:, j - 1], X.T)))) / len(X)\n",
        "        print(fb)\n",
        "        fb_grad = np.gradient(fb)\n",
        "        hessian = np.empty((fb.ndim, fb.ndim) + fb.shape, dtype=fb.dtype) \n",
        "        for k, grad_k in enumerate(fb_grad):\n",
        "            # iterate over dimensions\n",
        "            # apply gradient again to every component of the first derivative.\n",
        "            tmp_grad = np.gradient(grad_k) \n",
        "            for l, grad_kl in enumerate(tmp_grad):\n",
        "                hessian[k, l, :, :] = grad_kl\n",
        "        print(fb_grad)\n",
        "        B[:,j] = B[:, j - 1] - hessian * fb_grad\n",
        "    return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_oxnDeIIawG"
      },
      "source": [
        "### Task 2\n",
        "Generate simulated data, so we can test the code. Take $p=5$ and $n=1000$. Plot the estimation error for each column of `B`. Compute the estimation error as $v_k = \\| \\beta_k - \\beta^* \\|_2^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuVx6w2nIawH"
      },
      "source": [
        "#fill in code here\n",
        "n=1000\n",
        "p=5\n",
        "betastar = np.ones(5)/np.sqrt(5)\n",
        "X = np.random.randn(n,p)\n",
        "y = np.zeros(n)\n",
        "y[:round(n/2)] = 1\n",
        "np.random.shuffle(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MERV5RrLIawJ"
      },
      "source": [
        "Biterates = logreg(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K67FXujcIawW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "v = np.sqrt(((Biterates.T - betastar) ** 2) / n)\n",
        "plt.plot(np.log(v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVw2SxcBFe0F"
      },
      "source": [
        "v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KSGQjgXFq3H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}